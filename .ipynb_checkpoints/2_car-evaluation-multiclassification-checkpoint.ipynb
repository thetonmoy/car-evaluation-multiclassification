{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "31c896a5-bcfb-4a74-ad8e-49b435fd2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2e218138-0edb-4cbb-a70d-f54872c60c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "car_evaluation = fetch_ucirepo(id=19)  \n",
    "\n",
    "X = car_evaluation.data.features \n",
    "y = car_evaluation.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0904ffee-2809-41cb-8402-f93d3eb36a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_acc  class_good  class_unacc  class_vgood\n",
      "0         False       False         True        False\n",
      "1         False       False         True        False\n",
      "2         False       False         True        False\n",
      "3         False       False         True        False\n",
      "4         False       False         True        False\n",
      "...         ...         ...          ...          ...\n",
      "1723      False        True        False        False\n",
      "1724      False       False        False         True\n",
      "1725      False       False         True        False\n",
      "1726      False        True        False        False\n",
      "1727      False       False        False         True\n",
      "\n",
      "[1728 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "X_encoded = pd.get_dummies(X) # One hot encoding\n",
    "y_encoded = pd.get_dummies(y) # One hot encoding\n",
    "\n",
    "# print(X_encoded)\n",
    "print(y_encoded)\n",
    "\n",
    "# print(type(X))\n",
    "# print(type(y))\n",
    "\n",
    "# print(X_encoded.shape)\n",
    "# print(y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fda07c25-1e71-42ae-b94b-ab734a9854f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data split, 70% training and 30% temp (temp = validation + test)\n",
    "X_train_encoded, X_temp_encoded, y_train_encoded, y_temp_encoded = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# 30% temp data into 15% validation and 15% test\n",
    "X_val_encoded, X_test_encoded, y_val_encoded, y_test_encoded = train_test_split(X_temp_encoded, y_temp_encoded, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "44f321c6-d04f-42d4-b4e1-60439af2eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Check if y_true is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.to_numpy()  # Convert y_true to a NumPy array\n",
    "\n",
    "    # Check if y_pred is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.to_numpy()  # Convert y_pred to a NumPy array\n",
    "\n",
    "    # Check if y_true is already flattened (1D)\n",
    "    if y_true.ndim > 1:\n",
    "        y_true = y_true.flatten()  # Flatten if it has more than 1 dimension\n",
    "\n",
    "    # Check if y_pred is already flattened (1D)\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = y_pred.flatten()  # Flatten if it has more than 1 dimension\n",
    "\n",
    "    # Find unique class names in y_true and determine the number of unique classes\n",
    "    class_names = np.unique(y_true)  # Get unique class names from y_true\n",
    "    unique_classes = class_names.size  # Count the number of unique classes\n",
    "\n",
    "    # Initialize a confusion matrix with zeros, sized based on the number of unique classes\n",
    "    confusion_matrix = np.zeros((unique_classes, unique_classes), dtype=int)\n",
    "\n",
    "    # Map class names to indices for easy lookup\n",
    "    class_name_to_index = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "    # Count occurrences of actual vs predicted labels\n",
    "    for actual, predicted in zip(y_true, y_pred):\n",
    "        # Ensure actual and predicted are scalar values, not arrays\n",
    "        actual = actual.item() if isinstance(actual, np.ndarray) else actual\n",
    "        predicted = predicted.item() if isinstance(predicted, np.ndarray) else predicted\n",
    "        \n",
    "        # Find the index for the actual and predicted class\n",
    "        actual_index = class_name_to_index[actual]\n",
    "        predicted_index = class_name_to_index[predicted]\n",
    "\n",
    "        # Increment the appropriate cell in the confusion matrix\n",
    "        confusion_matrix[actual_index, predicted_index] += 1\n",
    "\n",
    "    # Print the confusion matrix row by row\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    for row in confusion_matrix:\n",
    "        print(\" \".join(map(str, row)))  # Print each row of the confusion matrix\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "    \n",
    "    # Accuracy Calculation\n",
    "    \n",
    "    # Sum of the diagonal elements (correct predictions)\n",
    "    correct_predictions = np.trace(confusion_matrix)  # np.trace() gives the sum of diagonal elements\n",
    "\n",
    "    # Total number of predictions (sum of all elements in the matrix)\n",
    "    total_predictions = np.sum(confusion_matrix)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"\\nAccuracy: {accuracy}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "        \n",
    "    # Precision Calculation\n",
    "\n",
    "    def calculate_precision(confusion_matrix, class_names):\n",
    "        precision = {}\n",
    "        \n",
    "        # Iterate over each class to calculate its precision\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            # True Positive (TP) is the value in the diagonal for that class\n",
    "            true_positive = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Positive (FP) is the sum of the column (excluding the diagonal)\n",
    "            false_positive = np.sum(confusion_matrix[:, i]) - true_positive\n",
    "            \n",
    "            # Precision for the current class\n",
    "            precision[class_name] = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0        \n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    precision = calculate_precision(confusion_matrix, class_names)\n",
    "    print(\"\\nPrecision for each class:\")\n",
    "    for class_name, precision_value in precision.items():\n",
    "        print(f\"{class_name}: {precision_value:}\")\n",
    "\n",
    "    total_precision = sum(precision.values())\n",
    "    print(f\"\\nMacro precision: {total_precision / unique_classes}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "    \n",
    "    # Recall Calculation\n",
    "    \n",
    "    def calculate_recall(confusion_matrix, class_names):\n",
    "        recall = {}\n",
    "        \n",
    "        # Iterate over each class to calculate its recall\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            # True Positive (TP) is the value in the diagonal for that class\n",
    "            true_positive = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Negative (FN) is the sum of the row (excluding the diagonal)\n",
    "            false_negative = np.sum(confusion_matrix[i, :]) - true_positive\n",
    "            \n",
    "            # Recall for the current class\n",
    "            recall[class_name] = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0        \n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    recall = calculate_recall(confusion_matrix, class_names)\n",
    "    print(\"\\nRecall for each class:\")\n",
    "    for class_name, recall_value in recall.items():\n",
    "        print(f\"{class_name}: {recall_value:.4f}\")\n",
    "\n",
    "    total_recall = sum(recall.values())\n",
    "    print(f\"\\nMacro recall: {total_recall / unique_classes}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "\n",
    "    # F1 Score Calculation\n",
    "    \n",
    "    def calculate_f1_score(precision, recall):\n",
    "        f1_scores = {}\n",
    "    \n",
    "        # Calculate F1 score for each class\n",
    "        for class_name in precision.keys():\n",
    "            p = precision[class_name]\n",
    "            r = recall[class_name]\n",
    "            \n",
    "            # Calculate F1 score for the class, handling cases where p + r = 0\n",
    "            f1_scores[class_name] = (2 * p * r) / (p + r) if (p + r) != 0 else 0\n",
    "        \n",
    "        return f1_scores\n",
    "    \n",
    "    f1_scores = calculate_f1_score(precision, recall)\n",
    "    print(\"\\nF1 Score for each class:\")\n",
    "    for class_name, f1_value in f1_scores.items():\n",
    "        print(f\"{class_name}: {f1_value:}\")\n",
    "        \n",
    "    # Macro F1 Score Calculation\n",
    "    total_f1 = sum(f1_scores.values())  # Sum of F1 scores for each class\n",
    "    macro_f1 = total_f1 / len(f1_scores)  # Average F1 score across all classes\n",
    "    \n",
    "    print(f\"\\nMacro F1 score: {macro_f1:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1963a9ec-06f7-480b-8579-9aed964a7424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Training Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "3575 52\n",
      "106 1103\n",
      "\n",
      "Accuracy: 0.967328370554177\n",
      "\n",
      "Precision for each class:\n",
      "False: 0.9712034773159468\n",
      "True: 0.9549783549783549\n",
      "\n",
      "Macro precision: 0.9630909161471508\n",
      "\n",
      "Recall for each class:\n",
      "False: 0.9857\n",
      "True: 0.9123\n",
      "\n",
      "Macro recall: 0.948993658671078\n",
      "\n",
      "F1 Score for each class:\n",
      "False: 0.9783798576902026\n",
      "True: 0.9331641285956006\n",
      "\n",
      "Macro F1 score: 0.9557719931429016\n",
      "------------------------------\n",
      "Validation Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "759 18\n",
      "41 218\n",
      "\n",
      "Accuracy: 0.943050193050193\n",
      "\n",
      "Precision for each class:\n",
      "False: 0.94875\n",
      "True: 0.923728813559322\n",
      "\n",
      "Macro precision: 0.936239406779661\n",
      "\n",
      "Recall for each class:\n",
      "False: 0.9768\n",
      "True: 0.8417\n",
      "\n",
      "Macro recall: 0.9092664092664093\n",
      "\n",
      "F1 Score for each class:\n",
      "False: 0.9625871908687381\n",
      "True: 0.8808080808080808\n",
      "\n",
      "Macro F1 score: 0.9216976358384095\n",
      "------------------------------\n",
      "Test Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "758 22\n",
      "41 219\n",
      "\n",
      "Accuracy: 0.9394230769230769\n",
      "\n",
      "Precision for each class:\n",
      "False: 0.9486858573216521\n",
      "True: 0.9087136929460581\n",
      "\n",
      "Macro precision: 0.928699775133855\n",
      "\n",
      "Recall for each class:\n",
      "False: 0.9718\n",
      "True: 0.8423\n",
      "\n",
      "Macro recall: 0.907051282051282\n",
      "\n",
      "F1 Score for each class:\n",
      "False: 0.9601013299556681\n",
      "True: 0.8742514970059879\n",
      "\n",
      "Macro F1 score: 0.917176413480828\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "y_train_pred_knn = knn.predict(X_train_encoded)\n",
    "y_val_pred_knn = knn.predict(X_val_encoded)\n",
    "y_test_pred_knn = knn.predict(X_test_encoded)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Training Set Metrics:\")\n",
    "calculate_metrics(y_train_encoded, y_train_pred_knn)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Validation Set Metrics:\")\n",
    "calculate_metrics(y_val_encoded, y_val_pred_knn)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Test Set Metrics:\")\n",
    "calculate_metrics(y_test_encoded, y_test_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e5c9354b-3b13-4a64-94c8-9dcf0c1a9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_numeric = y_train_encoded.idxmax(axis=1).map({\n",
    "    'class_acc': 0,\n",
    "    'class_good': 1,\n",
    "    'class_unacc': 2,\n",
    "    'class_vgood': 3\n",
    "})\n",
    "\n",
    "y_val_numeric = y_val_encoded.idxmax(axis=1).map({\n",
    "    'class_acc': 0,\n",
    "    'class_good': 1,\n",
    "    'class_unacc': 2,\n",
    "    'class_vgood': 3\n",
    "})\n",
    "\n",
    "y_test_numeric = y_test_encoded.idxmax(axis=1).map({\n",
    "    'class_acc': 0,\n",
    "    'class_good': 1,\n",
    "    'class_unacc': 2,\n",
    "    'class_vgood': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "322d7c22-d00f-4ac5-a968-07bcab70c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Training Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "192 6 68 0\n",
      "35 15 0 0\n",
      "31 1 820 0\n",
      "17 1 0 23\n",
      "\n",
      "Accuracy: 0.8684863523573201\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.6981818181818182\n",
      "1: 0.6521739130434783\n",
      "2: 0.9234234234234234\n",
      "3: 1.0\n",
      "\n",
      "Macro precision: 0.81844478866218\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.7218\n",
      "1: 0.3000\n",
      "2: 0.9624\n",
      "3: 0.5610\n",
      "\n",
      "Macro recall: 0.636305358897071\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.7097966728280962\n",
      "1: 0.410958904109589\n",
      "2: 0.942528735632184\n",
      "3: 0.71875\n",
      "\n",
      "Macro F1 score: 0.6955085781424672\n",
      "------------------------------\n",
      "Validation Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "38 2 17 0\n",
      "8 2 0 0\n",
      "4 0 174 0\n",
      "9 0 0 5\n",
      "\n",
      "Accuracy: 0.8455598455598455\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.6440677966101694\n",
      "1: 0.5\n",
      "2: 0.9109947643979057\n",
      "3: 1.0\n",
      "\n",
      "Macro precision: 0.7637656402520188\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.6667\n",
      "1: 0.2000\n",
      "2: 0.9775\n",
      "3: 0.3571\n",
      "\n",
      "Macro recall: 0.5503344034242911\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.6551724137931034\n",
      "1: 0.28571428571428575\n",
      "2: 0.943089430894309\n",
      "3: 0.5263157894736842\n",
      "\n",
      "Macro F1 score: 0.6025729799688456\n",
      "------------------------------\n",
      "Test Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "33 3 25 0\n",
      "6 3 0 0\n",
      "5 1 174 0\n",
      "7 0 0 3\n",
      "\n",
      "Accuracy: 0.8192307692307692\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.6470588235294118\n",
      "1: 0.42857142857142855\n",
      "2: 0.8743718592964824\n",
      "3: 1.0\n",
      "\n",
      "Macro precision: 0.7375005278493307\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.5410\n",
      "1: 0.3333\n",
      "2: 0.9667\n",
      "3: 0.3000\n",
      "\n",
      "Macro recall: 0.5352459016393443\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.5892857142857144\n",
      "1: 0.375\n",
      "2: 0.9182058047493403\n",
      "3: 0.4615384615384615\n",
      "\n",
      "Macro F1 score: 0.5860074951433791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_encoded, y_train_numeric)\n",
    "\n",
    "y_train_pred_nb = nb_classifier.predict(X_train_encoded)\n",
    "y_val_pred_nb = nb_classifier.predict(X_val_encoded)\n",
    "y_test_pred_nb = nb_classifier.predict(X_test_encoded)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Training Set Metrics:\")\n",
    "calculate_metrics(y_train_numeric, y_train_pred_nb)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Validation Set Metrics:\")\n",
    "calculate_metrics(y_val_numeric, y_val_pred_nb)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Test Set Metrics:\")\n",
    "calculate_metrics(y_test_numeric, y_test_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bf0acb10-d433-4ca0-b95d-ca0890433bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Training Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "237 7 21 1\n",
      "0 49 0 1\n",
      "26 3 823 0\n",
      "0 0 0 41\n",
      "\n",
      "Accuracy: 0.9511993382961125\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.9011406844106464\n",
      "1: 0.8305084745762712\n",
      "2: 0.9751184834123223\n",
      "3: 0.9534883720930233\n",
      "\n",
      "Macro precision: 0.9150640036230657\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.8910\n",
      "1: 0.9800\n",
      "2: 0.9660\n",
      "3: 1.0000\n",
      "\n",
      "Macro recall: 0.9592349712308941\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.8960302457466919\n",
      "1: 0.8990825688073395\n",
      "2: 0.9705188679245284\n",
      "3: 0.9761904761904763\n",
      "\n",
      "Macro F1 score: 0.935455539667259\n",
      "------------------------------\n",
      "Validation Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "45 3 8 1\n",
      "0 8 0 2\n",
      "5 0 173 0\n",
      "0 0 0 14\n",
      "\n",
      "Accuracy: 0.9266409266409267\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.9\n",
      "1: 0.7272727272727273\n",
      "2: 0.9558011049723757\n",
      "3: 0.8235294117647058\n",
      "\n",
      "Macro precision: 0.8516508110024523\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.7895\n",
      "1: 0.8000\n",
      "2: 0.9719\n",
      "3: 1.0000\n",
      "\n",
      "Macro recall: 0.8903459491425192\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.8411214953271027\n",
      "1: 0.761904761904762\n",
      "2: 0.9637883008356547\n",
      "3: 0.9032258064516129\n",
      "\n",
      "Macro F1 score: 0.8675100911297831\n",
      "------------------------------\n",
      "Test Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "52 8 1 0\n",
      "0 9 0 0\n",
      "7 1 172 0\n",
      "0 0 0 10\n",
      "\n",
      "Accuracy: 0.9346153846153846\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.8813559322033898\n",
      "1: 0.5\n",
      "2: 0.9942196531791907\n",
      "3: 1.0\n",
      "\n",
      "Macro precision: 0.843893896345645\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.8525\n",
      "1: 1.0000\n",
      "2: 0.9556\n",
      "3: 1.0000\n",
      "\n",
      "Macro recall: 0.9520036429872496\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.8666666666666666\n",
      "1: 0.6666666666666666\n",
      "2: 0.9745042492917847\n",
      "3: 1.0\n",
      "\n",
      "Macro F1 score: 0.8769593956562795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)  # 'linear', 'rbf'\n",
    "svm_classifier.fit(X_train_encoded, y_train_numeric)\n",
    "\n",
    "y_train_pred_svm = svm_classifier.predict(X_train_encoded)\n",
    "y_val_pred_svm = svm_classifier.predict(X_val_encoded)\n",
    "y_test_pred_svm = svm_classifier.predict(X_test_encoded)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Training Set Metrics:\")\n",
    "calculate_metrics(y_train_numeric, y_train_pred_svm)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Validation Set Metrics:\")\n",
    "calculate_metrics(y_val_numeric, y_val_pred_svm)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Test Set Metrics:\")\n",
    "calculate_metrics(y_test_numeric, y_test_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "19bbcf71-4089-4fdc-b552-733685ad55ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "ZeroR Classifier - Training Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "0 0 266 0\n",
      "0 0 50 0\n",
      "0 0 852 0\n",
      "0 0 41 0\n",
      "\n",
      "Accuracy: 0.7047146401985112\n",
      "\n",
      "Precision for each class:\n",
      "0: 0\n",
      "1: 0\n",
      "2: 0.7047146401985112\n",
      "3: 0\n",
      "\n",
      "Macro precision: 0.1761786600496278\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.0000\n",
      "1: 0.0000\n",
      "2: 1.0000\n",
      "3: 0.0000\n",
      "\n",
      "Macro recall: 0.25\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0\n",
      "1: 0\n",
      "2: 0.826783114992722\n",
      "3: 0\n",
      "\n",
      "Macro F1 score: 0.2066957787481805\n",
      "------------------------------\n",
      "ZeroR Classifier - Validation Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "0 0 57 0\n",
      "0 0 10 0\n",
      "0 0 178 0\n",
      "0 0 14 0\n",
      "\n",
      "Accuracy: 0.6872586872586872\n",
      "\n",
      "Precision for each class:\n",
      "0: 0\n",
      "1: 0\n",
      "2: 0.6872586872586872\n",
      "3: 0\n",
      "\n",
      "Macro precision: 0.1718146718146718\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.0000\n",
      "1: 0.0000\n",
      "2: 1.0000\n",
      "3: 0.0000\n",
      "\n",
      "Macro recall: 0.25\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0\n",
      "1: 0\n",
      "2: 0.8146453089244851\n",
      "3: 0\n",
      "\n",
      "Macro F1 score: 0.20366132723112126\n",
      "------------------------------\n",
      "ZeroR Classifier - Test Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "0 0 61 0\n",
      "0 0 9 0\n",
      "0 0 180 0\n",
      "0 0 10 0\n",
      "\n",
      "Accuracy: 0.6923076923076923\n",
      "\n",
      "Precision for each class:\n",
      "0: 0\n",
      "1: 0\n",
      "2: 0.6923076923076923\n",
      "3: 0\n",
      "\n",
      "Macro precision: 0.17307692307692307\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.0000\n",
      "1: 0.0000\n",
      "2: 1.0000\n",
      "3: 0.0000\n",
      "\n",
      "Macro recall: 0.25\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0\n",
      "1: 0\n",
      "2: 0.8181818181818181\n",
      "3: 0\n",
      "\n",
      "Macro F1 score: 0.20454545454545453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "zero_r_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "zero_r_classifier.fit(X_train_encoded, y_train_numeric)\n",
    "\n",
    "y_train_pred_zero_r = zero_r_classifier.predict(X_train_encoded)\n",
    "y_val_pred_zero_r = zero_r_classifier.predict(X_val_encoded)\n",
    "y_test_pred_zero_r = zero_r_classifier.predict(X_test_encoded)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"ZeroR Classifier - Training Set Metrics:\")\n",
    "calculate_metrics(y_train_numeric, y_train_pred_zero_r)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"ZeroR Classifier - Validation Set Metrics:\")\n",
    "calculate_metrics(y_val_numeric, y_val_pred_zero_r)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"ZeroR Classifier - Test Set Metrics:\")\n",
    "calculate_metrics(y_test_numeric, y_test_pred_zero_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fd8ea77f-cbaa-4942-854e-352b7edd25b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "OneR Classifier - Training Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "266 0 0 0\n",
      "50 0 0 0\n",
      "852 0 0 0\n",
      "41 0 0 0\n",
      "\n",
      "Accuracy: 0.22001654259718775\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.22001654259718775\n",
      "1: 0\n",
      "2: 0\n",
      "3: 0\n",
      "\n",
      "Macro precision: 0.05500413564929694\n",
      "\n",
      "Recall for each class:\n",
      "0: 1.0000\n",
      "1: 0.0000\n",
      "2: 0.0000\n",
      "3: 0.0000\n",
      "\n",
      "Macro recall: 0.25\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.3606779661016949\n",
      "1: 0\n",
      "2: 0\n",
      "3: 0\n",
      "\n",
      "Macro F1 score: 0.09016949152542372\n",
      "------------------------------\n",
      "OneR Classifier - Validation Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "57 0 0 0\n",
      "10 0 0 0\n",
      "178 0 0 0\n",
      "14 0 0 0\n",
      "\n",
      "Accuracy: 0.22007722007722008\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.22007722007722008\n",
      "1: 0\n",
      "2: 0\n",
      "3: 0\n",
      "\n",
      "Macro precision: 0.05501930501930502\n",
      "\n",
      "Recall for each class:\n",
      "0: 1.0000\n",
      "1: 0.0000\n",
      "2: 0.0000\n",
      "3: 0.0000\n",
      "\n",
      "Macro recall: 0.25\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.36075949367088606\n",
      "1: 0\n",
      "2: 0\n",
      "3: 0\n",
      "\n",
      "Macro F1 score: 0.09018987341772151\n",
      "------------------------------\n",
      "OneR Classifier - Test Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "61 0 0 0\n",
      "9 0 0 0\n",
      "180 0 0 0\n",
      "10 0 0 0\n",
      "\n",
      "Accuracy: 0.23461538461538461\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.23461538461538461\n",
      "1: 0\n",
      "2: 0\n",
      "3: 0\n",
      "\n",
      "Macro precision: 0.058653846153846154\n",
      "\n",
      "Recall for each class:\n",
      "0: 1.0000\n",
      "1: 0.0000\n",
      "2: 0.0000\n",
      "3: 0.0000\n",
      "\n",
      "Macro recall: 0.25\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.38006230529595014\n",
      "1: 0\n",
      "2: 0\n",
      "3: 0\n",
      "\n",
      "Macro F1 score: 0.09501557632398754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class OneRClassifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.rules = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "        self.rules = {}\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            feature_values = X[:, feature_index]\n",
    "            rule_accuracy = {}\n",
    "            for value in np.unique(feature_values):\n",
    "                predicted_class = y[feature_values == value].mode()[0]\n",
    "                rule_accuracy[value] = (y[feature_values == value] == predicted_class).mean()\n",
    "            best_value = max(rule_accuracy, key=rule_accuracy.get)\n",
    "            self.rules[feature_index] = best_value\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "        predictions = []\n",
    "        for row in X:\n",
    "            row_predictions = [self.rules.get(i, None) for i in range(len(row))]\n",
    "            most_frequent_class = max(set(row_predictions), key=row_predictions.count)  # majority vote for the row\n",
    "            predictions.append(most_frequent_class)\n",
    "        return np.array(predictions)\n",
    "\n",
    "one_r_classifier = OneRClassifier()\n",
    "one_r_classifier.fit(X_train_encoded, y_train_numeric)\n",
    "\n",
    "y_train_pred_one_r = one_r_classifier.predict(X_train_encoded)\n",
    "y_val_pred_one_r = one_r_classifier.predict(X_val_encoded)\n",
    "y_test_pred_one_r = one_r_classifier.predict(X_test_encoded)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"OneR Classifier - Training Set Metrics:\")\n",
    "calculate_metrics(y_train_numeric, y_train_pred_one_r)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"OneR Classifier - Validation Set Metrics:\")\n",
    "calculate_metrics(y_val_numeric, y_val_pred_one_r)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"OneR Classifier - Test Set Metrics:\")\n",
    "calculate_metrics(y_test_numeric, y_test_pred_one_r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
