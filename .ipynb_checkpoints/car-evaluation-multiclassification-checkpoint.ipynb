{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d9e2c5c8-56d8-4abc-bc6d-82a0e016f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9ca3e7c9-fbdb-4fa0-b8b0-b744372b698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "car_evaluation = fetch_ucirepo(id=19) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = car_evaluation.data.features \n",
    "y = car_evaluation.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b1db308b-31fe-4016-be51-4606464585cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data split, 70% training and 30% temp (temp = validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 30% temp data into 15% validation and 15% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d71e9350-49f2-46f6-979d-ecd7f99b47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import numpy as np\n",
    "\n",
    "# def calculate_metrics(y_true, y_pred):\n",
    "#     acc_acc = acc_good = acc_unacc = acc_vgood = 0\n",
    "#     good_acc = good_good = good_unacc = good_vgood = 0\n",
    "#     unacc_acc = unacc_good = unacc_unacc = unacc_vgood = 0\n",
    "#     vgood_acc = vgood_good = vgood_unacc = vgood_vgood = 0\n",
    "\n",
    "#     y_true = y_true.to_numpy()\n",
    "    \n",
    "#     for i in range(len(y_true)):\n",
    "#         actual = y_true[i]\n",
    "#         predicted = y_pred[i]\n",
    "        \n",
    "#         if actual[0] and predicted[0]:\n",
    "#             acc_acc += 1\n",
    "#         elif actual[0] and predicted[1]:\n",
    "#             acc_good += 1\n",
    "#         elif actual[0] and predicted[2]:\n",
    "#             acc_unacc += 1\n",
    "#         elif actual[0] and predicted[3]:\n",
    "#             acc_vgood += 1\n",
    "#         elif actual[1] and predicted[0]:\n",
    "#             good_acc += 1\n",
    "#         elif actual[1] and predicted[1]:\n",
    "#             good_good += 1\n",
    "#         elif actual[1] and predicted[2]:\n",
    "#             good_unacc += 1\n",
    "#         elif actual[1] and predicted[3]:\n",
    "#             good_vgood += 1\n",
    "#         elif actual[2] and predicted[0]:\n",
    "#             unacc_acc += 1\n",
    "#         elif actual[2] and predicted[1]:\n",
    "#             unacc_good += 1\n",
    "#         elif actual[2] and predicted[2]:\n",
    "#             unacc_unacc += 1\n",
    "#         elif actual[2] and predicted[3]:\n",
    "#             unacc_vgood += 1\n",
    "#         elif actual[3] and predicted[0]:\n",
    "#             vgood_acc += 1\n",
    "#         elif actual[3] and predicted[1]:\n",
    "#             vgood_good += 1\n",
    "#         elif actual[3] and predicted[2]:\n",
    "#             vgood_unacc += 1\n",
    "#         elif actual[3] and predicted[3]:\n",
    "#             vgood_vgood += 1\n",
    "\n",
    "#     confusion_matrix = [\n",
    "#         [acc_acc, acc_good, acc_unacc, acc_vgood],\n",
    "#         [good_acc, good_good, good_unacc, good_vgood],\n",
    "#         [unacc_acc, unacc_good, unacc_unacc, unacc_vgood],\n",
    "#         [vgood_acc, vgood_good, vgood_unacc, vgood_vgood]\n",
    "#     ]\n",
    "\n",
    "#     # Accuracy\n",
    "#     total_correct = acc_acc + good_good + unacc_unacc + vgood_vgood\n",
    "#     total_predictions = sum(sum(row) for row in confusion_matrix)\n",
    "#     accuracy = total_correct / total_predictions\n",
    "\n",
    "#     # Precision calculations\n",
    "#     if (acc_acc + good_acc + unacc_acc + vgood_acc) > 0:\n",
    "#         Precision_of_acc = acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc)\n",
    "#     else:\n",
    "#         Precision_of_acc = 0\n",
    "\n",
    "#     if (acc_good + good_good + unacc_good + vgood_good) > 0:\n",
    "#         Precision_of_good = good_good / (acc_good + good_good + unacc_good + vgood_good)\n",
    "#     else:\n",
    "#         Precision_of_good = 0\n",
    "\n",
    "#     if (acc_unacc + good_unacc + unacc_unacc + vgood_unacc) > 0:\n",
    "#         Precision_of_unacc = unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc)\n",
    "#     else:\n",
    "#         Precision_of_unacc = 0\n",
    "\n",
    "#     if (acc_vgood + good_vgood + unacc_vgood + vgood_vgood) > 0:\n",
    "#         Precision_of_vgood = vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood)\n",
    "#     else:\n",
    "#         Precision_of_vgood = 0\n",
    "        \n",
    "#     # Precision_of_acc = acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc)\n",
    "#     # Precision_of_good = good_good / (acc_good + good_good + unacc_good + vgood_good)\n",
    "#     # Precision_of_unacc = unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc)\n",
    "#     # Precision_of_vgood = vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood)\n",
    "#     average_precision = (Precision_of_acc + Precision_of_good + Precision_of_unacc + Precision_of_vgood) / 4.0\n",
    "\n",
    "#     # Recall calculations\n",
    "#     if (acc_acc + acc_good + acc_unacc + acc_vgood) > 0:\n",
    "#         Recall_of_acc = acc_acc / (acc_acc + acc_good + acc_unacc + acc_vgood)\n",
    "#     else:\n",
    "#         Recall_of_acc = 0\n",
    "\n",
    "#     if (good_acc + good_good + good_unacc + good_vgood) > 0:\n",
    "#         Recall_of_good = good_good / (good_acc + good_good + good_unacc + good_vgood)\n",
    "#     else:\n",
    "#         Recall_of_good = 0\n",
    "\n",
    "#     if (unacc_acc + unacc_good + unacc_unacc + unacc_vgood) > 0:\n",
    "#         Recall_of_unacc = unacc_unacc / (unacc_acc + unacc_good + unacc_unacc + unacc_vgood)\n",
    "#     else:\n",
    "#         Recall_of_unacc = 0\n",
    "\n",
    "#     if (vgood_acc + vgood_good + vgood_unacc + vgood_vgood) > 0:\n",
    "#          Recall_of_vgood = vgood_vgood / (vgood_acc + vgood_good + vgood_unacc + vgood_vgood)\n",
    "#     else:\n",
    "#         Recall_of_vgood = 0\n",
    "        \n",
    "#     # Recall_of_acc = acc_acc / (acc_acc + acc_good + acc_unacc + acc_vgood)\n",
    "#     # Recall_of_good = good_good / (good_acc + good_good + good_unacc + good_vgood)\n",
    "#     # Recall_of_unacc = unacc_unacc / (unacc_acc + unacc_good + unacc_unacc + unacc_vgood)\n",
    "#     # Recall_of_vgood = vgood_vgood / (vgood_acc + vgood_good + vgood_unacc + vgood_vgood)\n",
    "#     average_recall = (Recall_of_acc + Recall_of_good + Recall_of_unacc + Recall_of_vgood) / 4.0\n",
    "\n",
    "#     # F1 Score\n",
    "#     average_f1_score = 2 * ((average_precision * average_recall) / (average_precision + average_recall))\n",
    "\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     for row in confusion_matrix:\n",
    "#         print(row)\n",
    "\n",
    "#     print(f\"Accuracy: {accuracy}\")\n",
    "#     print(f\"Average Precision: {average_precision}\")\n",
    "#     print(f\"Average Recall: {average_recall}\")\n",
    "#     print(f\"Average F1 Score: {average_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4152f45b-21ea-4957-801f-be193d5dea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Check if y_true is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.to_numpy()  # Convert y_true to a NumPy array\n",
    "\n",
    "    # Check if y_pred is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.to_numpy()  # Convert y_pred to a NumPy array\n",
    "\n",
    "    # Check if y_true is already flattened (1D)\n",
    "    if y_true.ndim > 1:\n",
    "        y_true = y_true.flatten()  # Flatten if it has more than 1 dimension\n",
    "\n",
    "    # Check if y_pred is already flattened (1D)\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = y_pred.flatten()  # Flatten if it has more than 1 dimension\n",
    "\n",
    "    # Find unique class names in y_true and determine the number of unique classes\n",
    "    class_names = np.unique(y_true)  # Get unique class names from y_true\n",
    "    unique_classes = class_names.size  # Count the number of unique classes\n",
    "\n",
    "    # Initialize a confusion matrix with zeros, sized based on the number of unique classes\n",
    "    confusion_matrix = np.zeros((unique_classes, unique_classes), dtype=int)\n",
    "\n",
    "    # Map class names to indices for easy lookup\n",
    "    class_name_to_index = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "    # Count occurrences of actual vs predicted labels\n",
    "    for actual, predicted in zip(y_true, y_pred):\n",
    "        # Ensure actual and predicted are scalar values, not arrays\n",
    "        actual = actual.item() if isinstance(actual, np.ndarray) else actual\n",
    "        predicted = predicted.item() if isinstance(predicted, np.ndarray) else predicted\n",
    "        \n",
    "        # Find the index for the actual and predicted class\n",
    "        actual_index = class_name_to_index[actual]\n",
    "        predicted_index = class_name_to_index[predicted]\n",
    "\n",
    "        # Increment the appropriate cell in the confusion matrix\n",
    "        confusion_matrix[actual_index, predicted_index] += 1\n",
    "\n",
    "    # Print the confusion matrix row by row\n",
    "    print(\"Confusion matrix:\")\n",
    "    for row in confusion_matrix:\n",
    "        print(\" \".join(map(str, row)))  # Print each row of the confusion matrix\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "    \n",
    "    # Accuracy Calculation\n",
    "    \n",
    "    # Sum of the diagonal elements (correct predictions)\n",
    "    correct_predictions = np.trace(confusion_matrix)  # np.trace() gives the sum of diagonal elements\n",
    "\n",
    "    # Total number of predictions (sum of all elements in the matrix)\n",
    "    total_predictions = np.sum(confusion_matrix)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "        \n",
    "    # Precision Calculation\n",
    "\n",
    "    def calculate_precision(confusion_matrix, class_names):\n",
    "        precision = {}\n",
    "        \n",
    "        # Iterate over each class to calculate its precision\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            # True Positive (TP) is the value in the diagonal for that class\n",
    "            true_positive = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Positive (FP) is the sum of the column (excluding the diagonal)\n",
    "            false_positive = np.sum(confusion_matrix[:, i]) - true_positive\n",
    "            \n",
    "            # Precision for the current class\n",
    "            precision[class_name] = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0\n",
    "            \n",
    "        return precision\n",
    "    \n",
    "    precision = calculate_precision(confusion_matrix, class_names)\n",
    "    print(\"Precision for each class:\")\n",
    "    for class_name, precision_value in precision.items():\n",
    "        print(f\"{class_name}: {precision_value:.4f}\")\n",
    "\n",
    "    return confusion_matrix, class_names, accuracy, precision\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9ec1763f-a256-4d28-9c3b-d28e1925184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X_train: (1209, 6)\n",
      "Type of y_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of y_train: (1209, 1)\n",
      "Type of X_temp: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X_temp: (519, 6)\n",
      "Type of y_temp: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of y_temp: (519, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of X_train:\", type(X_train))\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Type of y_train:\", type(y_train))\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Type of X_temp:\", type(X_temp))\n",
    "print(\"Shape of X_temp:\", X_temp.shape)\n",
    "print(\"Type of y_temp:\", type(y_temp))\n",
    "print(\"Shape of y_temp:\", y_temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d9f60-8794-4a7a-8bda-ba1c5d540f7f",
   "metadata": {},
   "source": [
    "**ZeroR** <br>\n",
    "In a multiclass classification setting, ZeroR will look at the target values in the training data, count the frequencies of each class, and select the most frequent class as its prediction for all instances, regardless of the input features. <br>\n",
    "ZeroR doesn't learn from input features, so it only needs the training data to calculate the most frequent class. There's no need to tune hyperparameters, so a validation set isn't required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27289235-d031-4e39-8885-5c4ada97e996",
   "metadata": {},
   "source": [
    "**ZeroR Model Implementation**  \n",
    "This cell implements the ZeroR algorithm, which predicts the most frequent class from the training data, ignoring all feature information. It calculates the mode of the target variable (`y_train`) and uses this most frequent class to make predictions for the training, validation, and test sets. The accuracy of the model is then evaluated by comparing the predicted labels with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b27da015-e4fa-4cd3-b6e5-d2b512390433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroR Model Accuracy:\n",
      "Training Accuracy: 0.7047146401985112\n",
      "Validation Accuracy: 0.6872586872586872\n",
      "Test Accuracy: 0.6923076923076923\n",
      "------------------------------\n",
      "Validation Set:\n",
      "Confusion matrix:\n",
      "0 0 57 0\n",
      "0 0 10 0\n",
      "0 0 178 0\n",
      "0 0 14 0\n",
      "Accuracy: 0.6872586872586872\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'calculate_precision' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m calculate_metrics(y_val, y_val_pred)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[192], line 64\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# --- --- --- --- --- ---\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Precision Calculation\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m precision \u001b[38;5;241m=\u001b[39m calculate_precision(confusion_matrix, class_names)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision for each class:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_name, precision_value \u001b[38;5;129;01min\u001b[39;00m precision\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'calculate_precision' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get the most frequent class from the training set\n",
    "most_frequent_class = y_train.mode(axis=0).iloc[0]  # Get the most frequent class label\n",
    "\n",
    "# Predict the most frequent class for all samples in the training, validation, and test sets\n",
    "y_train_pred = np.full(y_train.shape[0], most_frequent_class)\n",
    "y_val_pred = np.full(y_val.shape[0], most_frequent_class)\n",
    "y_test_pred = np.full(y_test.shape[0], most_frequent_class)\n",
    "\n",
    "# Calculate accuracy using the true labels and the predicted labels\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print ZeroR model accuracy\n",
    "print(\"ZeroR Model Accuracy:\")\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Validation Set:\")\n",
    "calculate_metrics(y_val, y_val_pred)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Test Set:\")\n",
    "calculate_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3da128-9f81-4d72-bb7a-c02bb22a7a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
