{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9e2c5c8-56d8-4abc-bc6d-82a0e016f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ca3e7c9-fbdb-4fa0-b8b0-b744372b698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "car_evaluation = fetch_ucirepo(id=19) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = car_evaluation.data.features \n",
    "y = car_evaluation.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1db308b-31fe-4016-be51-4606464585cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data split, 70% training and 30% temp (temp = validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 30% temp data into 15% validation and 15% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4152f45b-21ea-4957-801f-be193d5dea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Check if y_true is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.to_numpy()  # Convert y_true to a NumPy array\n",
    "\n",
    "    # Check if y_pred is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.to_numpy()  # Convert y_pred to a NumPy array\n",
    "\n",
    "    # Check if y_true is already flattened (1D)\n",
    "    if y_true.ndim > 1:\n",
    "        y_true = y_true.flatten()  # Flatten if it has more than 1 dimension\n",
    "\n",
    "    # Check if y_pred is already flattened (1D)\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = y_pred.flatten()  # Flatten if it has more than 1 dimension\n",
    "\n",
    "    # Find unique class names in y_true and determine the number of unique classes\n",
    "    class_names = np.unique(y_true)  # Get unique class names from y_true\n",
    "    unique_classes = class_names.size  # Count the number of unique classes\n",
    "\n",
    "    # Initialize a confusion matrix with zeros, sized based on the number of unique classes\n",
    "    confusion_matrix = np.zeros((unique_classes, unique_classes), dtype=int)\n",
    "\n",
    "    # Map class names to indices for easy lookup\n",
    "    class_name_to_index = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "    # Count occurrences of actual vs predicted labels\n",
    "    for actual, predicted in zip(y_true, y_pred):\n",
    "        # Ensure actual and predicted are scalar values, not arrays\n",
    "        actual = actual.item() if isinstance(actual, np.ndarray) else actual\n",
    "        predicted = predicted.item() if isinstance(predicted, np.ndarray) else predicted\n",
    "        \n",
    "        # Find the index for the actual and predicted class\n",
    "        actual_index = class_name_to_index[actual]\n",
    "        predicted_index = class_name_to_index[predicted]\n",
    "\n",
    "        # Increment the appropriate cell in the confusion matrix\n",
    "        confusion_matrix[actual_index, predicted_index] += 1\n",
    "\n",
    "    # Print the confusion matrix row by row\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    for row in confusion_matrix:\n",
    "        print(\" \".join(map(str, row)))  # Print each row of the confusion matrix\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "    \n",
    "    # Accuracy Calculation\n",
    "    \n",
    "    # Sum of the diagonal elements (correct predictions)\n",
    "    correct_predictions = np.trace(confusion_matrix)  # np.trace() gives the sum of diagonal elements\n",
    "\n",
    "    # Total number of predictions (sum of all elements in the matrix)\n",
    "    total_predictions = np.sum(confusion_matrix)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"\\nAccuracy: {accuracy}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "        \n",
    "    # Precision Calculation\n",
    "\n",
    "    def calculate_precision(confusion_matrix, class_names):\n",
    "        precision = {}\n",
    "        \n",
    "        # Iterate over each class to calculate its precision\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            # True Positive (TP) is the value in the diagonal for that class\n",
    "            true_positive = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Positive (FP) is the sum of the column (excluding the diagonal)\n",
    "            false_positive = np.sum(confusion_matrix[:, i]) - true_positive\n",
    "            \n",
    "            # Precision for the current class\n",
    "            precision[class_name] = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0        \n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    precision = calculate_precision(confusion_matrix, class_names)\n",
    "    print(\"\\nPrecision for each class:\")\n",
    "    for class_name, precision_value in precision.items():\n",
    "        print(f\"{class_name}: {precision_value:}\")\n",
    "\n",
    "    total_precision = sum(precision.values())\n",
    "    print(f\"\\nMacro precision: {total_precision / unique_classes}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "    \n",
    "    # Recall Calculation\n",
    "    \n",
    "    def calculate_recall(confusion_matrix, class_names):\n",
    "        recall = {}\n",
    "        \n",
    "        # Iterate over each class to calculate its recall\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            # True Positive (TP) is the value in the diagonal for that class\n",
    "            true_positive = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Negative (FN) is the sum of the row (excluding the diagonal)\n",
    "            false_negative = np.sum(confusion_matrix[i, :]) - true_positive\n",
    "            \n",
    "            # Recall for the current class\n",
    "            recall[class_name] = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0        \n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    recall = calculate_recall(confusion_matrix, class_names)\n",
    "    print(\"\\nRecall for each class:\")\n",
    "    for class_name, recall_value in recall.items():\n",
    "        print(f\"{class_name}: {recall_value:.4f}\")\n",
    "\n",
    "    total_recall = sum(recall.values())\n",
    "    print(f\"\\nMacro recall: {total_recall / unique_classes}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "\n",
    "    # F1 Score Calculation\n",
    "    \n",
    "    def calculate_f1_score(precision, recall):\n",
    "        f1_scores = {}\n",
    "    \n",
    "        # Calculate F1 score for each class\n",
    "        for class_name in precision.keys():\n",
    "            p = precision[class_name]\n",
    "            r = recall[class_name]\n",
    "            \n",
    "            # Calculate F1 score for the class, handling cases where p + r = 0\n",
    "            f1_scores[class_name] = (2 * p * r) / (p + r) if (p + r) != 0 else 0\n",
    "        \n",
    "        return f1_scores\n",
    "    \n",
    "    f1_scores = calculate_f1_score(precision, recall)\n",
    "    print(\"\\nF1 Score for each class:\")\n",
    "    for class_name, f1_value in f1_scores.items():\n",
    "        print(f\"{class_name}: {f1_value:}\")\n",
    "        \n",
    "    # Macro F1 Score Calculation\n",
    "    total_f1 = sum(f1_scores.values())  # Sum of F1 scores for each class\n",
    "    macro_f1 = total_f1 / len(f1_scores)  # Average F1 score across all classes\n",
    "    \n",
    "    print(f\"\\nMacro F1 score: {macro_f1:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ec1763f-a256-4d28-9c3b-d28e1925184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X_train: (1209, 6)\n",
      "Type of y_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of y_train: (1209, 1)\n",
      "Type of X_temp: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X_temp: (519, 6)\n",
      "Type of y_temp: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of y_temp: (519, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of X_train:\", type(X_train))\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Type of y_train:\", type(y_train))\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Type of X_temp:\", type(X_temp))\n",
    "print(\"Shape of X_temp:\", X_temp.shape)\n",
    "print(\"Type of y_temp:\", type(y_temp))\n",
    "print(\"Shape of y_temp:\", y_temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d9f60-8794-4a7a-8bda-ba1c5d540f7f",
   "metadata": {},
   "source": [
    "**ZeroR** <br>\n",
    "In a multiclass classification setting, ZeroR will look at the target values in the training data, count the frequencies of each class, and select the most frequent class as its prediction for all instances, regardless of the input features. <br>\n",
    "ZeroR doesn't learn from input features, so it only needs the training data to calculate the most frequent class. There's no need to tune hyperparameters, so a validation set isn't required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27289235-d031-4e39-8885-5c4ada97e996",
   "metadata": {},
   "source": [
    "**ZeroR Model Implementation**  \n",
    "This cell implements the ZeroR algorithm, which predicts the most frequent class from the training data, ignoring all feature information. It calculates the mode of the target variable (`y_train`) and uses this most frequent class to make predictions for the training, validation, and test sets. The accuracy of the model is then evaluated by comparing the predicted labels with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b27da015-e4fa-4cd3-b6e5-d2b512390433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroR Model Accuracy:\n",
      "Training Accuracy: 0.7047146401985112\n",
      "Validation Accuracy: 0.6872586872586872\n",
      "Test Accuracy: 0.6923076923076923\n",
      "------------------------------\n",
      "Validation Set:\n",
      "\n",
      "Confusion matrix:\n",
      "0 0 57 0\n",
      "0 0 10 0\n",
      "0 0 178 0\n",
      "0 0 14 0\n",
      "\n",
      "Accuracy: 0.6872586872586872\n",
      "\n",
      "Precision for each class:\n",
      "acc: 0\n",
      "good: 0\n",
      "unacc: 0.6872586872586872\n",
      "vgood: 0\n",
      "\n",
      "Macro precision: 0.1718146718146718\n",
      "\n",
      "Recall for each class:\n",
      "acc: 0.0000\n",
      "good: 0.0000\n",
      "unacc: 1.0000\n",
      "vgood: 0.0000\n",
      "\n",
      "Macro recall: 0.25\n",
      "\n",
      "F1 Score for each class:\n",
      "acc: 0\n",
      "good: 0\n",
      "unacc: 0.8146453089244851\n",
      "vgood: 0\n",
      "\n",
      "Macro F1 score: 0.20366132723112126\n",
      "------------------------------\n",
      "Test Set:\n",
      "\n",
      "Confusion matrix:\n",
      "0 0 61 0\n",
      "0 0 9 0\n",
      "0 0 180 0\n",
      "0 0 10 0\n",
      "\n",
      "Accuracy: 0.6923076923076923\n",
      "\n",
      "Precision for each class:\n",
      "acc: 0\n",
      "good: 0\n",
      "unacc: 0.6923076923076923\n",
      "vgood: 0\n",
      "\n",
      "Macro precision: 0.17307692307692307\n",
      "\n",
      "Recall for each class:\n",
      "acc: 0.0000\n",
      "good: 0.0000\n",
      "unacc: 1.0000\n",
      "vgood: 0.0000\n",
      "\n",
      "Macro recall: 0.25\n",
      "\n",
      "F1 Score for each class:\n",
      "acc: 0\n",
      "good: 0\n",
      "unacc: 0.8181818181818181\n",
      "vgood: 0\n",
      "\n",
      "Macro F1 score: 0.20454545454545453\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get the most frequent class from the training set\n",
    "most_frequent_class = y_train.mode(axis=0).iloc[0]  # Get the most frequent class label\n",
    "\n",
    "# Predict the most frequent class for all samples in the training, validation, and test sets\n",
    "y_train_pred = np.full(y_train.shape[0], most_frequent_class)\n",
    "y_val_pred = np.full(y_val.shape[0], most_frequent_class)\n",
    "y_test_pred = np.full(y_test.shape[0], most_frequent_class)\n",
    "\n",
    "# Calculate accuracy using the true labels and the predicted labels\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print ZeroR model accuracy\n",
    "print(\"ZeroR Model Accuracy:\")\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Validation Set:\")\n",
    "calculate_metrics(y_val, y_val_pred)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Test Set:\")\n",
    "calculate_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3da128-9f81-4d72-bb7a-c02bb22a7a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
