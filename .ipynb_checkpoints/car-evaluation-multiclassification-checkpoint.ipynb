{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9e2c5c8-56d8-4abc-bc6d-82a0e016f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9ca3e7c9-fbdb-4fa0-b8b0-b744372b698d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>vgood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>vgood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1728 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class\n",
       "0     unacc\n",
       "1     unacc\n",
       "2     unacc\n",
       "3     unacc\n",
       "4     unacc\n",
       "...     ...\n",
       "1723   good\n",
       "1724  vgood\n",
       "1725  unacc\n",
       "1726   good\n",
       "1727  vgood\n",
       "\n",
       "[1728 rows x 1 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "car_evaluation = fetch_ucirepo(id=19) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = car_evaluation.data.features \n",
    "y = car_evaluation.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1db308b-31fe-4016-be51-4606464585cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data split, 70% training and 30% temp (temp = validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 30% temp data into 15% validation and 15% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d71e9350-49f2-46f6-979d-ecd7f99b47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import numpy as np\n",
    "\n",
    "# def calculate_metrics(y_true, y_pred):\n",
    "#     acc_acc = acc_good = acc_unacc = acc_vgood = 0\n",
    "#     good_acc = good_good = good_unacc = good_vgood = 0\n",
    "#     unacc_acc = unacc_good = unacc_unacc = unacc_vgood = 0\n",
    "#     vgood_acc = vgood_good = vgood_unacc = vgood_vgood = 0\n",
    "\n",
    "#     y_true = y_true.to_numpy()\n",
    "    \n",
    "#     for i in range(len(y_true)):\n",
    "#         actual = y_true[i]\n",
    "#         predicted = y_pred[i]\n",
    "        \n",
    "#         if actual[0] and predicted[0]:\n",
    "#             acc_acc += 1\n",
    "#         elif actual[0] and predicted[1]:\n",
    "#             acc_good += 1\n",
    "#         elif actual[0] and predicted[2]:\n",
    "#             acc_unacc += 1\n",
    "#         elif actual[0] and predicted[3]:\n",
    "#             acc_vgood += 1\n",
    "#         elif actual[1] and predicted[0]:\n",
    "#             good_acc += 1\n",
    "#         elif actual[1] and predicted[1]:\n",
    "#             good_good += 1\n",
    "#         elif actual[1] and predicted[2]:\n",
    "#             good_unacc += 1\n",
    "#         elif actual[1] and predicted[3]:\n",
    "#             good_vgood += 1\n",
    "#         elif actual[2] and predicted[0]:\n",
    "#             unacc_acc += 1\n",
    "#         elif actual[2] and predicted[1]:\n",
    "#             unacc_good += 1\n",
    "#         elif actual[2] and predicted[2]:\n",
    "#             unacc_unacc += 1\n",
    "#         elif actual[2] and predicted[3]:\n",
    "#             unacc_vgood += 1\n",
    "#         elif actual[3] and predicted[0]:\n",
    "#             vgood_acc += 1\n",
    "#         elif actual[3] and predicted[1]:\n",
    "#             vgood_good += 1\n",
    "#         elif actual[3] and predicted[2]:\n",
    "#             vgood_unacc += 1\n",
    "#         elif actual[3] and predicted[3]:\n",
    "#             vgood_vgood += 1\n",
    "\n",
    "#     confusion_matrix = [\n",
    "#         [acc_acc, acc_good, acc_unacc, acc_vgood],\n",
    "#         [good_acc, good_good, good_unacc, good_vgood],\n",
    "#         [unacc_acc, unacc_good, unacc_unacc, unacc_vgood],\n",
    "#         [vgood_acc, vgood_good, vgood_unacc, vgood_vgood]\n",
    "#     ]\n",
    "\n",
    "#     # Accuracy\n",
    "#     total_correct = acc_acc + good_good + unacc_unacc + vgood_vgood\n",
    "#     total_predictions = sum(sum(row) for row in confusion_matrix)\n",
    "#     accuracy = total_correct / total_predictions\n",
    "\n",
    "#     # Precision calculations\n",
    "#     if (acc_acc + good_acc + unacc_acc + vgood_acc) > 0:\n",
    "#         Precision_of_acc = acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc)\n",
    "#     else:\n",
    "#         Precision_of_acc = 0\n",
    "\n",
    "#     if (acc_good + good_good + unacc_good + vgood_good) > 0:\n",
    "#         Precision_of_good = good_good / (acc_good + good_good + unacc_good + vgood_good)\n",
    "#     else:\n",
    "#         Precision_of_good = 0\n",
    "\n",
    "#     if (acc_unacc + good_unacc + unacc_unacc + vgood_unacc) > 0:\n",
    "#         Precision_of_unacc = unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc)\n",
    "#     else:\n",
    "#         Precision_of_unacc = 0\n",
    "\n",
    "#     if (acc_vgood + good_vgood + unacc_vgood + vgood_vgood) > 0:\n",
    "#         Precision_of_vgood = vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood)\n",
    "#     else:\n",
    "#         Precision_of_vgood = 0\n",
    "        \n",
    "#     # Precision_of_acc = acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc)\n",
    "#     # Precision_of_good = good_good / (acc_good + good_good + unacc_good + vgood_good)\n",
    "#     # Precision_of_unacc = unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc)\n",
    "#     # Precision_of_vgood = vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood)\n",
    "#     average_precision = (Precision_of_acc + Precision_of_good + Precision_of_unacc + Precision_of_vgood) / 4.0\n",
    "\n",
    "#     # Recall calculations\n",
    "#     if (acc_acc + acc_good + acc_unacc + acc_vgood) > 0:\n",
    "#         Recall_of_acc = acc_acc / (acc_acc + acc_good + acc_unacc + acc_vgood)\n",
    "#     else:\n",
    "#         Recall_of_acc = 0\n",
    "\n",
    "#     if (good_acc + good_good + good_unacc + good_vgood) > 0:\n",
    "#         Recall_of_good = good_good / (good_acc + good_good + good_unacc + good_vgood)\n",
    "#     else:\n",
    "#         Recall_of_good = 0\n",
    "\n",
    "#     if (unacc_acc + unacc_good + unacc_unacc + unacc_vgood) > 0:\n",
    "#         Recall_of_unacc = unacc_unacc / (unacc_acc + unacc_good + unacc_unacc + unacc_vgood)\n",
    "#     else:\n",
    "#         Recall_of_unacc = 0\n",
    "\n",
    "#     if (vgood_acc + vgood_good + vgood_unacc + vgood_vgood) > 0:\n",
    "#          Recall_of_vgood = vgood_vgood / (vgood_acc + vgood_good + vgood_unacc + vgood_vgood)\n",
    "#     else:\n",
    "#         Recall_of_vgood = 0\n",
    "        \n",
    "#     # Recall_of_acc = acc_acc / (acc_acc + acc_good + acc_unacc + acc_vgood)\n",
    "#     # Recall_of_good = good_good / (good_acc + good_good + good_unacc + good_vgood)\n",
    "#     # Recall_of_unacc = unacc_unacc / (unacc_acc + unacc_good + unacc_unacc + unacc_vgood)\n",
    "#     # Recall_of_vgood = vgood_vgood / (vgood_acc + vgood_good + vgood_unacc + vgood_vgood)\n",
    "#     average_recall = (Recall_of_acc + Recall_of_good + Recall_of_unacc + Recall_of_vgood) / 4.0\n",
    "\n",
    "#     # F1 Score\n",
    "#     average_f1_score = 2 * ((average_precision * average_recall) / (average_precision + average_recall))\n",
    "\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     for row in confusion_matrix:\n",
    "#         print(row)\n",
    "\n",
    "#     print(f\"Accuracy: {accuracy}\")\n",
    "#     print(f\"Average Precision: {average_precision}\")\n",
    "#     print(f\"Average Recall: {average_recall}\")\n",
    "#     print(f\"Average F1 Score: {average_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4152f45b-21ea-4957-801f-be193d5dea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Check if y_true is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.to_numpy()  # Convert y_true to a NumPy array\n",
    "\n",
    "    # Check if y_pred is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.to_numpy()  # Convert y_pred to a NumPy array\n",
    "\n",
    "    # # Shape before flattening\n",
    "    # print(y_true.shape)\n",
    "    # print(y_pred.shape)\n",
    "    \n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # # Shape after flattening\n",
    "    # print(y_true.shape)\n",
    "    # print(y_pred.shape)\n",
    "\n",
    "    # Find unique class names in y_true and determine the number of unique classes\n",
    "    class_names = np.unique(y_true)  # Get unique class names from y_true\n",
    "    unique_classes = class_names.size  # Count the number of unique classes\n",
    "\n",
    "    # Initialize a confusion matrix with zeros, sized based on the number of unique classes\n",
    "    confusion_matrix = np.zeros((unique_classes, unique_classes), dtype=int)\n",
    "\n",
    "    # Map class names to indices for easy lookup\n",
    "    class_name_to_index = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "    # Count occurrences of actual vs predicted labels\n",
    "    for actual, predicted in zip(y_true, y_pred):\n",
    "        # Ensure actual and predicted are scalar values, not arrays\n",
    "        actual = actual.item() if isinstance(actual, np.ndarray) else actual\n",
    "        predicted = predicted.item() if isinstance(predicted, np.ndarray) else predicted\n",
    "        \n",
    "        # Find the index for the actual and predicted class\n",
    "        actual_index = class_name_to_index[actual]\n",
    "        predicted_index = class_name_to_index[predicted]\n",
    "\n",
    "        # Increment the appropriate cell in the confusion matrix\n",
    "        confusion_matrix[actual_index, predicted_index] += 1\n",
    "\n",
    "    # print(type(confusion_matrix))\n",
    "    # print(confusion_matrix.shape)\n",
    "\n",
    "    # Print the confusion matrix row by row\n",
    "    for row in confusion_matrix:\n",
    "        print(\" \".join(map(str, row)))  # Print each row of the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ec1763f-a256-4d28-9c3b-d28e1925184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X_train: (1209, 6)\n",
      "Type of y_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of y_train: (1209, 1)\n",
      "Type of X_temp: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of X_temp: (519, 6)\n",
      "Type of y_temp: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of y_temp: (519, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of X_train:\", type(X_train))\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Type of y_train:\", type(y_train))\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Type of X_temp:\", type(X_temp))\n",
    "print(\"Shape of X_temp:\", X_temp.shape)\n",
    "print(\"Type of y_temp:\", type(y_temp))\n",
    "print(\"Shape of y_temp:\", y_temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d9f60-8794-4a7a-8bda-ba1c5d540f7f",
   "metadata": {},
   "source": [
    "**ZeroR** <br>\n",
    "In a multiclass classification setting, ZeroR will look at the target values in the training data, count the frequencies of each class, and select the most frequent class as its prediction for all instances, regardless of the input features. <br>\n",
    "ZeroR doesn't learn from input features, so it only needs the training data to calculate the most frequent class. There's no need to tune hyperparameters, so a validation set isn't required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27289235-d031-4e39-8885-5c4ada97e996",
   "metadata": {},
   "source": [
    "**ZeroR Model Implementation**  \n",
    "This cell implements the ZeroR algorithm, which predicts the most frequent class from the training data, ignoring all feature information. It calculates the mode of the target variable (`y_train`) and uses this most frequent class to make predictions for the training, validation, and test sets. The accuracy of the model is then evaluated by comparing the predicted labels with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b27da015-e4fa-4cd3-b6e5-d2b512390433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroR Model Accuracy:\n",
      "Training Accuracy: 0.7047146401985112\n",
      "Validation Accuracy: 0.6872586872586872\n",
      "Test Accuracy: 0.6923076923076923\n",
      "------------------------------\n",
      "Validation Set:\n",
      "0 0 57 0\n",
      "0 0 10 0\n",
      "0 0 178 0\n",
      "0 0 14 0\n",
      "------------------------------\n",
      "Test Set:\n",
      "0 0 61 0\n",
      "0 0 9 0\n",
      "0 0 180 0\n",
      "0 0 10 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get the most frequent class from the training set\n",
    "most_frequent_class = y_train.mode(axis=0).iloc[0]  # Get the most frequent class label\n",
    "\n",
    "# Predict the most frequent class for all samples in the training, validation, and test sets\n",
    "y_train_pred = np.full(y_train.shape[0], most_frequent_class)\n",
    "y_val_pred = np.full(y_val.shape[0], most_frequent_class)\n",
    "y_test_pred = np.full(y_test.shape[0], most_frequent_class)\n",
    "\n",
    "# Calculate accuracy using the true labels and the predicted labels\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print ZeroR model accuracy\n",
    "print(\"ZeroR Model Accuracy:\")\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Validation Set:\")\n",
    "calculate_metrics(y_val, y_val_pred)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Test Set:\")\n",
    "calculate_metrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4bbb7f9e-aa7d-48af-90d6-e40d2607a8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing a confusion matrix of size unique_classes x unique_classes to store the results\n",
    "unique_classes = 5    \n",
    "confusion_matrix = np.zeros((unique_classes, unique_classes), dtype=int)\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3da128-9f81-4d72-bb7a-c02bb22a7a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
